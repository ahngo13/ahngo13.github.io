I"'C<h1 id="구성목표">구성목표</h1>

<ul>
  <li>마스터 노드 3개의 클러스터 구성</li>
  <li>가상 네트워크 (Calico) 적용</li>
</ul>

<h1 id="사전-작업하기">사전 작업하기</h1>

<h2 id="가상-머신-세팅">가상 머신 세팅</h2>

<ul>
  <li>Windows 10 Hyper-V로 3개의 가상머신 생성
    <ul>
      <li>OS: CentOS7 Minimal</li>
      <li>RAM : 2048MB</li>
      <li>HDD : 20GB</li>
      <li>CPU : 2</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/kuberneteshighlyavailableclusters.png" alt="/assets/img/kuberneteshighlyavailableclusters.png" /></p>

<h3 id="마스터-워커에-대한-가상머신-최소-요구-사항">마스터, 워커에 대한 가상머신 최소 요구 사항</h3>

<ul>
  <li>다음 중 하나를 실행하는 하나 이상의 머신 :
    <ul>
      <li>Ubuntu 16.04 이상</li>
      <li>Debian 9 이상</li>
      <li>CentOS 7</li>
      <li>RHEL (Red Hat Enterprise Linux) 7</li>
      <li>Fedora 25 이상</li>
      <li>HypriotOS v1.0.1 이상</li>
      <li>Flatcar Container Linux (2512.3.0으로 테스트 됨)</li>
    </ul>
  </li>
  <li>컴퓨터 당 2GB 이상의 RAM (그보다 적 으면 앱을위한 공간이 거의 남지 않음)</li>
  <li>CPU 2 개 이상</li>
  <li>클러스터의 모든 시스템 간의 전체 네트워크 연결 (공용 또는 사설 네트워크는 괜찮음)</li>
  <li>
    <p>모든 노드에 대한 고유 한 호스트 이름, MAC 주소 및 product_uuid. 자세한 내용은 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address">여기</a></p>

    <p>를 참조하십시오.</p>
  </li>
  <li>
    <p>컴퓨터에서 특정 포트가 열려 있습니다. 자세한 내용은 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports">여기</a></p>

    <p>를 참조하십시오.</p>
  </li>
  <li>
    <p>스왑이 비활성화되었습니다. 당신은 <strong>반드시</strong></p>

    <p>제대로 작동하려면 kubelet 위해서는 스왑을 사용하지 않도록 설정합니다.</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## 방화벽 해제 및 종료</span>
<span class="o">[</span>root@localhost ~]# systemctl disable firewalld <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>systemctl stop firewalld

<span class="c">## paging과 swap 기능 종료</span>
<span class="o">[</span>root@localhost ~]# swapoff <span class="nt">-a</span>

<span class="c">## 커널 속성 변경 (swap disable)</span>
<span class="o">[</span>root@localhost ~]# <span class="nb">echo </span>0 <span class="o">&gt;</span> /proc/sys/vm/swappiness

<span class="c">## swap을 하는 파일 시스템을 찾아 disable 처리</span>
<span class="o">[</span>root@localhost ~]# <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'/swap/ s/^#*/#/'</span> <span class="nt">-i</span> /etc/fstab

<span class="c">## RHEL, CentOS 7 기준 iptables 이슈로 인한 커널 매개변수 수정</span>
<span class="c">## iptables가 브리지 된 트래픽을 보게하기</span>
<span class="o">[</span>root@localhost ~]# <span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
</span><span class="no">EOF
</span><span class="c">## 수정된 매개변수 확인</span>
<span class="o">[</span>root@localhost ~]# sysctl <span class="nt">--system</span>
<span class="k">*</span> Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> 0
net.bridge.bridge-nf-call-iptables <span class="o">=</span> 0
net.bridge.bridge-nf-call-arptables <span class="o">=</span> 0
<span class="k">*</span> Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope <span class="o">=</span> 0
<span class="k">*</span> Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq <span class="o">=</span> 16
kernel.core_uses_pid <span class="o">=</span> 1
net.ipv4.conf.default.rp_filter <span class="o">=</span> 1
net.ipv4.conf.all.rp_filter <span class="o">=</span> 1
net.ipv4.conf.default.accept_source_route <span class="o">=</span> 0
net.ipv4.conf.all.accept_source_route <span class="o">=</span> 0
net.ipv4.conf.default.promote_secondaries <span class="o">=</span> 1
net.ipv4.conf.all.promote_secondaries <span class="o">=</span> 1
fs.protected_hardlinks <span class="o">=</span> 1
fs.protected_symlinks <span class="o">=</span> 1
<span class="k">*</span> Applying /etc/sysctl.d/99-sysctl.conf ...
<span class="k">*</span> Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> 1
net.bridge.bridge-nf-call-iptables <span class="o">=</span> 1
<span class="k">*</span> Applying /etc/sysctl.conf ... 

<span class="c">## br_netfilter 모듈 활성화</span>
<span class="o">[</span>root@localhost ~]# modprobe br_netfilter

<span class="c">## 모듈 추가 확인</span>
<span class="o">[</span>root@localhost ~]# lsmod | <span class="nb">grep </span>br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter

<span class="c">## 도커 설치</span>
<span class="o">[</span>root@localhost ~]# yum <span class="nb">install </span>docker-ec <span class="nt">-y</span>

<span class="c">## 부팅시 도커 자동으로 실행하게 설정</span>
<span class="o">[</span>root@localhost ~]# systemctl start docker.service
</code></pre></div></div>

<h2 id="docker-cgroup-setting">Docker cgroup setting</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; /etc/docker/daemon.json
{
"exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"storage-driver": "overlay2",
"storage-opts": [
"overlay2.override_kernel_check=true"
]
}
</span><span class="no">EOF

</span>systemctl daemon-reload
systemctl <span class="nb">enable </span>docker
systemctl restart docker

kubeadm reset <span class="nt">-f</span>
<span class="nb">rm</span> <span class="nt">-rf</span> /etc/kubernetes
<span class="nb">rm</span> <span class="nt">-rf</span> /etc/cni
<span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/etcd
<span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/kubenet
<span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/kubelet
<span class="nb">rm</span> <span class="nt">-rf</span> /root/.kube
</code></pre></div></div>

<ul>
  <li>쿠버네티스가 사용하는 포트들이므로 비워둬야 하며 모두 열려있어야 한다.</li>
</ul>

<p><img src="/assets/img/kuberneteshighlyavailableclusters1.png" alt="/assets/img/kuberneteshighlyavailableclusters1.png" /></p>

<h1 id="kubeadm-kubelet-및-kubectl-설치하기">kubeadm, kubelet 및 kubectl 설치하기</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-</span><span class="se">\$</span><span class="sh">basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
</span><span class="no">EOF

</span><span class="c">## SELinux(Security-Enhanced Linux) 리눅스 보안 모듈(액세스 권한 제어) 해당 기능 끄기</span>
<span class="o">[</span>root@localhost ~]# setenforce 0
<span class="o">[</span>root@localhost ~]# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/^SELINUX=enforcing$/SELINUX=permissive/'</span> /etc/selinux/config

<span class="c">## kubelet, kubeadm, kubectl 설치</span>
yum <span class="nb">install</span> <span class="nt">-y</span> kubelet kubeadm kubectl <span class="nt">--disableexcludes</span><span class="o">=</span>kubernetes

<span class="c">## kubelet 활성화</span>
<span class="o">[</span>root@localhost ~]# systemctl <span class="nb">enable</span> <span class="nt">--now</span> kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
</code></pre></div></div>

<h1 id="고가용성-클러스터-구축-시작">고가용성 클러스터 구축 시작</h1>

<h2 id="호스트네임-설정">호스트네임 설정</h2>

<ul>
  <li>각각의 가상머신 마다 hostname을 다르게 설정</li>
  <li>재부팅을 하지 않으면 이후 진행시 문제가 생길 수 있음</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## CentOS1</span>
<span class="o">[</span>root@localhost ~]# hostnamectl set-hostname node1

<span class="c">## CentOS2</span>
<span class="o">[</span>root@localhost ~]# hostnamectl set-hostname node2

<span class="c">## CentOS3</span>
<span class="o">[</span>root@localhost ~]# hostnamectl set-hostname node3
</code></pre></div></div>

<h2 id="kube-apiserver용-로드-밸런서-설치">kube-apiserver용 로드 밸런서 설치</h2>

<ul>
  <li>node1에만 haproxy 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yum <span class="nb">install </span>haproxy <span class="nt">-y</span>
</code></pre></div></div>

<ul>
  <li>node1 IP의 26643 포트로 전달받은 데이터를 node1 ~ node3의 6443 포트로 포워드 시키기</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@localhost ~]# vi /etc/haproxy/haproxy.cfg
frontend kubernetes-master-lb
<span class="nb">bind </span>0.0.0.0:26443
option tcplog
mode tcp
default_backend kubernetes-master-nodes

backend kubernetes-master-nodes
mode tcp
balance roundrobin
option tcp-check
option tcplog
server node1 172.31.218.71:6443 check <span class="c">## node1</span>
server node2 172.31.211.174:6443 check <span class="c">## node2</span>
server node3 172.31.220.154:6443 check <span class="c">## node3</span>

<span class="c">## haproxy 재시작</span>
<span class="o">[</span>root@localhost ~]# systemctl restart haproxy
</code></pre></div></div>

<h2 id="node1-클러스터-생성">node1 클러스터 생성</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm init <span class="nt">--control-plane-endpoint</span> <span class="s2">"172.31.218.71:26443"</span> <span class="se">\</span>
                <span class="nt">--upload-certs</span> <span class="se">\</span>
                <span class="nt">--pod-network-cidr</span><span class="o">=</span>192.168.0.0/16 <span class="c">## container의 아이피 할당 범위 설정 Calico on Kubernetes 기준 </span>
</code></pre></div></div>

<h2 id="node2-node3-image-pull">node2, node3 image pull</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm config images pull
</code></pre></div></div>

<h2 id="config-생성">config 생성</h2>

<ul>
  <li>권한이 필요하다면 사용자 디렉토리 하위에 <code class="language-plaintext highlighter-rouge">.kube/config</code> 생성 (master node에서 모두 실행)</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$HOME</span>/.kube
  <span class="nb">sudo cp</span> <span class="nt">-i</span> /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
  <span class="nb">sudo chown</span> <span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>:<span class="si">$(</span><span class="nb">id</span> <span class="nt">-g</span><span class="si">)</span> <span class="nv">$HOME</span>/.kube/config
</code></pre></div></div>

<h2 id="master-node-연결">master node 연결</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm <span class="nb">join </span>172.31.218.71:26443 <span class="nt">--token</span> q4nwtd.4gz9ertus6c1sb3s <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:4dcd823eb577d2efd08a9b269e3c546785b521a9146ae748d1420e5fcc51be9d <span class="se">\</span>
    <span class="nt">--control-plane</span> <span class="nt">--certificate-key</span> bea375424cf6fa409e8f4812d8da575922cc4c8c3520873263ea08f954d607cc
</code></pre></div></div>

<h2 id="work-node-연결">work node 연결</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm <span class="nb">join </span>172.31.218.71:26443 <span class="nt">--token</span> q4nwtd.4gz9ertus6c1sb3s <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:4dcd823eb577d2efd08a9b269e3c546785b521a9146ae748d1420e5fcc51be9d
</code></pre></div></div>

<h2 id="node-상태-확인">node 상태 확인</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@node1 ~]# kubectl get nodes
NAME    STATUS     ROLES    AGE     VERSION
node1   NotReady   master   3m14s   v1.19.1
node2   NotReady   master   68s     v1.19.1
node3   NotReady   master   64s     v1.19.1
</code></pre></div></div>

<h2 id="calico-yaml-파일-실행">Calico yaml 파일 실행</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create <span class="nt">-f</span> https://docs.projectcalico.org/manifests/custom-resources.yaml
</code></pre></div></div>

<h2 id="taint-삭제-pod-생성을-막으므로-삭제">taint 삭제 (pod 생성을 막으므로 삭제)</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl taint nodes <span class="nt">--all</span> node-role.kubernetes.io/master-
</code></pre></div></div>

<h2 id="pod-조회">pod 조회</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-A</code> : 전부 조회</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@node1 ~]# kubectl get pod <span class="nt">-A</span>
NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE
calico-system     calico-kube-controllers-69fbbf7967-ltqvx   1/1     Running   0          59s
calico-system     calico-node-pdd8n                          0/1     Running   0          59s
calico-system     calico-node-tgm7d                          0/1     Running   0          59s
calico-system     calico-node-v6zz6                          0/1     Running   0          59s
calico-system     calico-typha-7f44c5b874-mfhnd              1/1     Running   0          59s
kube-system       coredns-f9fd979d6-dbbqg                    1/1     Running   0          5m2s
kube-system       coredns-f9fd979d6-r2bs9                    1/1     Running   0          5m2s
kube-system       etcd-node1                                 1/1     Running   0          5m11s
kube-system       etcd-node2                                 1/1     Running   0          3m11s
kube-system       etcd-node3                                 1/1     Running   0          3m
kube-system       kube-apiserver-node1                       1/1     Running   0          5m11s
kube-system       kube-apiserver-node2                       1/1     Running   0          3m14s
kube-system       kube-apiserver-node3                       1/1     Running   0          101s
kube-system       kube-controller-manager-node1              1/1     Running   1          5m11s
kube-system       kube-controller-manager-node2              1/1     Running   0          3m10s
kube-system       kube-controller-manager-node3              1/1     Running   0          113s
kube-system       kube-proxy-6m5kb                           1/1     Running   0          2m33s
kube-system       kube-proxy-n8zpl                           1/1     Running   0          5m2s
kube-system       kube-proxy-xhhfc                           1/1     Running   0          3m15s
kube-system       kube-scheduler-node1                       1/1     Running   1          5m11s
kube-system       kube-scheduler-node2                       1/1     Running   0          3m14s
kube-system       kube-scheduler-node3                       1/1     Running   0          98s
tigera-operator   tigera-operator-646f758f9b-2l9hg           1/1     Running   0          69s
</code></pre></div></div>
:ET